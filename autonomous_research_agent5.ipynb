{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNEm+OSZju4hJc1XkoGBHJw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6fd2c3d0135a4a1fac6b11622e762943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3630672177214bc9857c5aec929a6eee",
              "IPY_MODEL_e1d4fbfebed54a36b36831a27cd6d933",
              "IPY_MODEL_f3916e6a36534a11bd73a4dec347c4dc"
            ],
            "layout": "IPY_MODEL_886e03fd07c94a37903c121c681468f9"
          }
        },
        "3630672177214bc9857c5aec929a6eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfd0cbdd40de402ebfa7ee9b0a32f5d1",
            "placeholder": "​",
            "style": "IPY_MODEL_9bfaa702c7f3444494684fcbc2f6aae9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e1d4fbfebed54a36b36831a27cd6d933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c12ae752cb214d859fb53862f16e0a47",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f403bd2c4bb4b7eaa8c065b2077c7f2",
            "value": 2
          }
        },
        "f3916e6a36534a11bd73a4dec347c4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88ee40a5d0104f5291a4a57e482a4906",
            "placeholder": "​",
            "style": "IPY_MODEL_67af230d9f2442bbbe624d2b7f5332a9",
            "value": " 2/2 [01:03&lt;00:00, 29.67s/it]"
          }
        },
        "886e03fd07c94a37903c121c681468f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfd0cbdd40de402ebfa7ee9b0a32f5d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bfaa702c7f3444494684fcbc2f6aae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c12ae752cb214d859fb53862f16e0a47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f403bd2c4bb4b7eaa8c065b2077c7f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88ee40a5d0104f5291a4a57e482a4906": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67af230d9f2442bbbe624d2b7f5332a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onehungrybird/Agentic_AI_travel_planner/blob/main/autonomous_research_agent5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fyw0dmsZR5GE",
        "outputId": "0b5f3cb4-e286-4d93-bbe2-564afdd8eeff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.47)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.18)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain_community) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain_community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (0.6.3)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.10.6)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.23.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.11/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb) (0.46.1)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.29.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install langchain_community\n",
        "!pip install -U transformers accelerate bitsandbytes --quiet\n",
        "!pip install -U bitsandbytes\n",
        "!pip install chromadb pypdf arxiv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "39fGIVo0SeXp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "# Set pad token if not already set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Function to generate responses\n",
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=300,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    # Decode the entire batch at once (outputs is already a tensor)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6fd2c3d0135a4a1fac6b11622e762943",
            "3630672177214bc9857c5aec929a6eee",
            "e1d4fbfebed54a36b36831a27cd6d933",
            "f3916e6a36534a11bd73a4dec347c4dc",
            "886e03fd07c94a37903c121c681468f9",
            "cfd0cbdd40de402ebfa7ee9b0a32f5d1",
            "9bfaa702c7f3444494684fcbc2f6aae9",
            "c12ae752cb214d859fb53862f16e0a47",
            "4f403bd2c4bb4b7eaa8c065b2077c7f2",
            "88ee40a5d0104f5291a4a57e482a4906",
            "67af230d9f2442bbbe624d2b7f5332a9"
          ]
        },
        "id": "n00NQ99sSfMT",
        "outputId": "825b6836-b1d1-4f05-ef14-3160337b7e9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fd2c3d0135a4a1fac6b11622e762943"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "print(generate_response(\"Explain the importance of data augmentation in deep learning.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlyST5szSqzx",
        "outputId": "e4f206eb-60f9-472b-b68c-31c5583627de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain the importance of data augmentation in deep learning.\n",
            "\n",
            "Data augmentation is a crucial technique used in deep learning to artificially increase the amount of data available for training. It is important for several reasons:\n",
            "\n",
            "1. Improving model performance: By augmenting the data, deep learning models can be trained on a larger and more diverse set of data, which can lead to improved performance. This is because the model learns to recognize patterns and features in the data more effectively, leading to better accuracy and generalization.\n",
            "\n",
            "2. Reducing overfitting: Overfitting occurs when a model becomes too specialized to the training data and is unable to generalize to new data. Data augmentation can help reduce overfitting by providing the model with a more diverse set of training data, which forces the model to learn more robust features.\n",
            "\n",
            "3. Increasing training efficiency: With data augmentation, deep learning models can be trained on a larger amount of data in the same amount of time. This is because the augmented data is synthetic, and the model can learn from it as if it were real data.\n",
            "\n",
            "4. Enabling transfer learning: Transfer learning is the ability to use a pre-trained model as a starting point for a new task. Data augmentation can help enable transfer learning by providing the pre-trained model with additional data to learn from, allowing it to better recognize patterns and features that are relevant to the new task.\n",
            "\n",
            "Overall, data augmentation is an important\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "\n",
        "def fetch_papers(query, max_results=3):\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "    )\n",
        "    papers = []\n",
        "    for result in search.results():\n",
        "        papers.append({\n",
        "            \"title\": result.title,\n",
        "            \"summary\": result.summary,\n",
        "            \"url\": result.entry_id\n",
        "        })\n",
        "    return papers"
      ],
      "metadata": {
        "id": "OD8w5tjrStD0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "papers = fetch_papers(\"deep learning for EEG\")\n",
        "for paper in papers:\n",
        "    print(f\"Title: {paper['title']}\\nSummary: {paper['summary']}\\nURL: {paper['url']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5XcuQtrXxqf",
        "outputId": "de920e14-f435-4237-c3c1-3d3a7d3b45f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-dd5836b357cf>:10: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Q-Insight: Understanding Image Quality via Visual Reinforcement Learning\n",
            "Summary: Image quality assessment (IQA) focuses on the perceptual visual quality of\n",
            "images, playing a crucial role in downstream tasks such as image\n",
            "reconstruction, compression, and generation. The rapid advancement of\n",
            "multi-modal large language models (MLLMs) has significantly broadened the scope\n",
            "of IQA, moving toward comprehensive image quality understanding that\n",
            "incorporates content analysis, degradation perception, and comparison reasoning\n",
            "beyond mere numerical scoring. Previous MLLM-based methods typically either\n",
            "generate numerical scores lacking interpretability or heavily rely on\n",
            "supervised fine-tuning (SFT) using large-scale annotated datasets to provide\n",
            "descriptive assessments, limiting their flexibility and applicability. In this\n",
            "paper, we propose Q-Insight, a reinforcement learning-based model built upon\n",
            "group relative policy optimization (GRPO), which demonstrates strong visual\n",
            "reasoning capability for image quality understanding while requiring only a\n",
            "limited amount of rating scores and degradation labels. By jointly optimizing\n",
            "score regression and degradation perception tasks with carefully designed\n",
            "reward functions, our approach effectively exploits their mutual benefits for\n",
            "enhanced performance. Extensive experiments demonstrate that Q-Insight\n",
            "substantially outperforms existing state-of-the-art methods in both score\n",
            "regression and degradation perception tasks, while exhibiting impressive\n",
            "zero-shot generalization to comparison reasoning tasks. Code will be available\n",
            "at https://github.com/lwq20020127/Q-Insight.\n",
            "URL: http://arxiv.org/abs/2503.22679v1\n",
            "\n",
            "Title: DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness\n",
            "Summary: Most 3D object generators focus on aesthetic quality, often neglecting\n",
            "physical constraints necessary in applications. One such constraint is that the\n",
            "3D object should be self-supporting, i.e., remains balanced under gravity.\n",
            "Prior approaches to generating stable 3D objects used differentiable physics\n",
            "simulators to optimize geometry at test-time, which is slow, unstable, and\n",
            "prone to local optima. Inspired by the literature on aligning generative models\n",
            "to external feedback, we propose Direct Simulation Optimization (DSO), a\n",
            "framework to use the feedback from a (non-differentiable) simulator to increase\n",
            "the likelihood that the 3D generator outputs stable 3D objects directly. We\n",
            "construct a dataset of 3D objects labeled with a stability score obtained from\n",
            "the physics simulator. We can then fine-tune the 3D generator using the\n",
            "stability score as the alignment metric, via direct preference optimization\n",
            "(DPO) or direct reward optimization (DRO), a novel objective, which we\n",
            "introduce, to align diffusion models without requiring pairwise preferences.\n",
            "Our experiments show that the fine-tuned feed-forward generator, using either\n",
            "DPO or DRO objective, is much faster and more likely to produce stable objects\n",
            "than test-time optimization. Notably, the DSO framework works even without any\n",
            "ground-truth 3D objects for training, allowing the 3D generator to self-improve\n",
            "by automatically collecting simulation feedback on its own outputs.\n",
            "URL: http://arxiv.org/abs/2503.22677v1\n",
            "\n",
            "Title: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions\n",
            "Summary: In this work, we introduce MedAgentSim, an open-source simulated clinical\n",
            "environment with doctor, patient, and measurement agents designed to evaluate\n",
            "and enhance LLM performance in dynamic diagnostic settings. Unlike prior\n",
            "approaches, our framework requires doctor agents to actively engage with\n",
            "patients through multi-turn conversations, requesting relevant medical\n",
            "examinations (e.g., temperature, blood pressure, ECG) and imaging results\n",
            "(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\n",
            "process. Additionally, we incorporate self improvement mechanisms that allow\n",
            "models to iteratively refine their diagnostic strategies. We enhance LLM\n",
            "performance in our simulated setting by integrating multi-agent discussions,\n",
            "chain-of-thought reasoning, and experience-based knowledge retrieval,\n",
            "facilitating progressive learning as doctor agents interact with more patients.\n",
            "We also introduce an evaluation benchmark for assessing the LLM's ability to\n",
            "engage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\n",
            "fully automated, it also supports a user-controlled mode, enabling human\n",
            "interaction with either the doctor or patient agent. Comprehensive evaluations\n",
            "in various simulated diagnostic scenarios demonstrate the effectiveness of our\n",
            "approach. Our code, simulation tool, and benchmark are available at\n",
            "\\href{https://medagentsim.netlify.app/}.\n",
            "URL: http://arxiv.org/abs/2503.22678v1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create Hugging Face pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=300,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Wrap in LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Define prompt template\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"paper_text\"],\n",
        "    template=\"Summarize this research paper in 3 bullet points: {paper_text}\"\n",
        ")\n",
        "\n",
        "# Create chain\n",
        "summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
        "\n",
        "# Run summarization\n",
        "summary = summary_chain.run({\"paper_text\": papers[0]['summary']})\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW03egrxX1U1",
        "outputId": "76a0bd2b-17bc-4720-8006-4349530ffa4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "<ipython-input-6-2e51c8df3515>:16: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n",
            "<ipython-input-6-2e51c8df3515>:28: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  summary = summary_chain.run({\"paper_text\": papers[0]['summary']})\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarize this research paper in 3 bullet points: Image quality assessment (IQA) focuses on the perceptual visual quality of\n",
            "images, playing a crucial role in downstream tasks such as image\n",
            "reconstruction, compression, and generation. The rapid advancement of\n",
            "multi-modal large language models (MLLMs) has significantly broadened the scope\n",
            "of IQA, moving toward comprehensive image quality understanding that\n",
            "incorporates content analysis, degradation perception, and comparison reasoning\n",
            "beyond mere numerical scoring. Previous MLLM-based methods typically either\n",
            "generate numerical scores lacking interpretability or heavily rely on\n",
            "supervised fine-tuning (SFT) using large-scale annotated datasets to provide\n",
            "descriptive assessments, limiting their flexibility and applicability. In this\n",
            "paper, we propose Q-Insight, a reinforcement learning-based model built upon\n",
            "group relative policy optimization (GRPO), which demonstrates strong visual\n",
            "reasoning capability for image quality understanding while requiring only a\n",
            "limited amount of rating scores and degradation labels. By jointly optimizing\n",
            "score regression and degradation perception tasks with carefully designed\n",
            "reward functions, our approach effectively exploits their mutual benefits for\n",
            "enhanced performance. Extensive experiments demonstrate that Q-Insight\n",
            "substantially outperforms existing state-of-the-art methods in both score\n",
            "regression and degradation perception tasks, while exhibiting impressive\n",
            "zero-shot generalization to comparison reasoning tasks. Code will be available\n",
            "at https://github.com/lwq20020127/Q-Insight.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hypothesis_prompt = PromptTemplate(\n",
        "    input_variables=[\"summary\"],\n",
        "    template=\"Based on this summary, suggest two new research hypotheses: {summary}\"\n",
        ")\n",
        "\n",
        "hypothesis_chain = LLMChain(llm=llm, prompt=hypothesis_prompt)\n",
        "hypothesis = hypothesis_chain.run({\"summary\": summary})\n",
        "\n",
        "print(\"\\nProposed Hypotheses:\\n\", hypothesis)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5Jhof3-ZgeL",
        "outputId": "93a6152b-d34e-4905-b956-c7481d79e28f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Proposed Hypotheses:\n",
            " Based on this summary, suggest two new research hypotheses: Summarize this research paper in 3 bullet points: Image quality assessment (IQA) focuses on the perceptual visual quality of\n",
            "images, playing a crucial role in downstream tasks such as image\n",
            "reconstruction, compression, and generation. The rapid advancement of\n",
            "multi-modal large language models (MLLMs) has significantly broadened the scope\n",
            "of IQA, moving toward comprehensive image quality understanding that\n",
            "incorporates content analysis, degradation perception, and comparison reasoning\n",
            "beyond mere numerical scoring. Previous MLLM-based methods typically either\n",
            "generate numerical scores lacking interpretability or heavily rely on\n",
            "supervised fine-tuning (SFT) using large-scale annotated datasets to provide\n",
            "descriptive assessments, limiting their flexibility and applicability. In this\n",
            "paper, we propose Q-Insight, a reinforcement learning-based model built upon\n",
            "group relative policy optimization (GRPO), which demonstrates strong visual\n",
            "reasoning capability for image quality understanding while requiring only a\n",
            "limited amount of rating scores and degradation labels. By jointly optimizing\n",
            "score regression and degradation perception tasks with carefully designed\n",
            "reward functions, our approach effectively exploits their mutual benefits for\n",
            "enhanced performance. Extensive experiments demonstrate that Q-Insight\n",
            "substantially outperforms existing state-of-the-art methods in both score\n",
            "regression and degradation perception tasks, while exhibiting impressive\n",
            "zero-shot generalization to comparison reasoning tasks. Code will be available\n",
            "at https://github.com/lwq20020127/Q-Insight.\n",
            "\n",
            "* Image quality assessment (IQA) is a crucial component of image processing tasks such as reconstruction, compression, and generation.\n",
            "* Multi-modal large language models (MLLMs) have broadened the scope of IQA, incorporating content analysis, degradation perception, and comparison reasoning beyond numerical scoring.\n",
            "* Q-Insight is a reinforcement learning-based model that demonstrates strong visual reasoning capability for image quality understanding while requiring only a limited amount of rating scores and degradation labels.\n",
            "\n",
            "Two new research hypotheses:\n",
            "\n",
            "1. The use of reinforcement learning-based models in image quality assessment can significantly improve the accuracy and flexibility of the process.\n",
            "2. By jointly optimizing score regression and degradation perception tasks, it is possible to enhance the performance of image quality assessment methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_prompt = PromptTemplate(\n",
        "    input_variables=[\"summary\", \"hypothesis\"],\n",
        "    template=\"Research Report\\n\\nSummary:\\n{summary}\\n\\nHypothesis:\\n{hypothesis}\\n\\nExperiment Results:\\n\\nConclusion:\\nWrite a conclusion based on these findings.\"\n",
        ")\n",
        "\n",
        "report_chain = LLMChain(llm=llm, prompt=report_prompt)\n",
        "report = report_chain.run({\"summary\": summary, \"hypothesis\": hypothesis})\n",
        "\n",
        "print(\"\\nFinal Research Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrWoOD7gb-Q7",
        "outputId": "121af93c-b653-42fa-8571-0b56c3881c54"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Research Report:\n",
            " Research Report\n",
            "\n",
            "Summary:\n",
            "Summarize this research paper in 3 bullet points: Image quality assessment (IQA) focuses on the perceptual visual quality of\n",
            "images, playing a crucial role in downstream tasks such as image\n",
            "reconstruction, compression, and generation. The rapid advancement of\n",
            "multi-modal large language models (MLLMs) has significantly broadened the scope\n",
            "of IQA, moving toward comprehensive image quality understanding that\n",
            "incorporates content analysis, degradation perception, and comparison reasoning\n",
            "beyond mere numerical scoring. Previous MLLM-based methods typically either\n",
            "generate numerical scores lacking interpretability or heavily rely on\n",
            "supervised fine-tuning (SFT) using large-scale annotated datasets to provide\n",
            "descriptive assessments, limiting their flexibility and applicability. In this\n",
            "paper, we propose Q-Insight, a reinforcement learning-based model built upon\n",
            "group relative policy optimization (GRPO), which demonstrates strong visual\n",
            "reasoning capability for image quality understanding while requiring only a\n",
            "limited amount of rating scores and degradation labels. By jointly optimizing\n",
            "score regression and degradation perception tasks with carefully designed\n",
            "reward functions, our approach effectively exploits their mutual benefits for\n",
            "enhanced performance. Extensive experiments demonstrate that Q-Insight\n",
            "substantially outperforms existing state-of-the-art methods in both score\n",
            "regression and degradation perception tasks, while exhibiting impressive\n",
            "zero-shot generalization to comparison reasoning tasks. Code will be available\n",
            "at https://github.com/lwq20020127/Q-Insight.\n",
            "\n",
            "Hypothesis:\n",
            "Based on this summary, suggest two new research hypotheses: Summarize this research paper in 3 bullet points: Image quality assessment (IQA) focuses on the perceptual visual quality of\n",
            "images, playing a crucial role in downstream tasks such as image\n",
            "reconstruction, compression, and generation. The rapid advancement of\n",
            "multi-modal large language models (MLLMs) has significantly broadened the scope\n",
            "of IQA, moving toward comprehensive image quality understanding that\n",
            "incorporates content analysis, degradation perception, and comparison reasoning\n",
            "beyond mere numerical scoring. Previous MLLM-based methods typically either\n",
            "generate numerical scores lacking interpretability or heavily rely on\n",
            "supervised fine-tuning (SFT) using large-scale annotated datasets to provide\n",
            "descriptive assessments, limiting their flexibility and applicability. In this\n",
            "paper, we propose Q-Insight, a reinforcement learning-based model built upon\n",
            "group relative policy optimization (GRPO), which demonstrates strong visual\n",
            "reasoning capability for image quality understanding while requiring only a\n",
            "limited amount of rating scores and degradation labels. By jointly optimizing\n",
            "score regression and degradation perception tasks with carefully designed\n",
            "reward functions, our approach effectively exploits their mutual benefits for\n",
            "enhanced performance. Extensive experiments demonstrate that Q-Insight\n",
            "substantially outperforms existing state-of-the-art methods in both score\n",
            "regression and degradation perception tasks, while exhibiting impressive\n",
            "zero-shot generalization to comparison reasoning tasks. Code will be available\n",
            "at https://github.com/lwq20020127/Q-Insight.\n",
            "\n",
            "* Image quality assessment (IQA) is a crucial component of image processing tasks such as reconstruction, compression, and generation.\n",
            "* Multi-modal large language models (MLLMs) have broadened the scope of IQA, incorporating content analysis, degradation perception, and comparison reasoning beyond numerical scoring.\n",
            "* Q-Insight is a reinforcement learning-based model that demonstrates strong visual reasoning capability for image quality understanding while requiring only a limited amount of rating scores and degradation labels.\n",
            "\n",
            "Two new research hypotheses:\n",
            "\n",
            "1. The use of reinforcement learning-based models in image quality assessment can significantly improve the accuracy and flexibility of the process.\n",
            "2. By jointly optimizing score regression and degradation perception tasks, it is possible to enhance the performance of image quality assessment methods.\n",
            "\n",
            "Experiment Results:\n",
            "\n",
            "Conclusion:\n",
            "Write a conclusion based on these findings.\n",
            "\n",
            "Conclusion:\n",
            "The proposed Q-Insight model demonstrates strong visual reasoning capability for image quality understanding, outperforming existing state-of-the-art methods in both score regression and degradation perception tasks. By jointly optimizing score regression and degradation perception tasks with carefully designed reward functions, Q-Insight effectively exploits their mutual benefits for enhanced performance. Furthermore, Q-Insight exhibits impressive zero-shot generalization to comparison reasoning tasks, suggesting its potential for broader applications in image processing tasks. Overall, these findings highlight the importance of reinforcement learning-based methods in image quality assessment and the potential for joint optimization of multiple tasks to improve performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qWQnzKT3cicu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}